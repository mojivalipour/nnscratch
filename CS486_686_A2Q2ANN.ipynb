{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS486-686-A2Q2ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mojivalipour/nnscratch/blob/master/CS486_686_A2Q2ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U36S_n53Z1BT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Design and Programming by Lead TA: Mojtaba Valipour @ Data Analytics Lab - UWaterloo.ca\n",
        "# COURSE: CS 486/686 - Artificial Intelligence - University of Waterloo - Spring 2020 - Alice Gao\n",
        "# Please let me know if you find any bugs in the code: m5valipo@uwaterloo.ca\n",
        "# The code will be available at https://github.com/mojivalipour/nnscratch\n",
        "# Version: 0.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2khwrVOaRbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement a neural network from scratch\n",
        "''' Sources:\n",
        "- http://neuralnetworksanddeeplearning.com/chap2.html\n",
        "'''\n",
        "print('Life is easy, you just need to do your best to find your place!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2phgK-b2aSTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn import datasets\n",
        "from sklearn.manifold import TSNE # visualization for data with more than two features\n",
        "from google.colab import files # if it is colab\n",
        "from os import path\n",
        "import pandas as pd\n",
        "import csv\n",
        "import copy\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OJZEFtBaS9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions\n",
        "def fixSeed(seed=1010):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msUa0eCkRmSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The hyper-parameters for the neural network\n",
        "\n",
        "nSamples = None # use None if you want to use full sample size\n",
        "# frogsSmall is the same dataset in Q1 that you have to use for comparision\n",
        "dataset = 'frogsSmall' # 2moons/frogsSmall/frogs\n",
        "noise = 0.05 # Noise in artificial datasets\n",
        "visNumSamples = 500 # number of samples to visualize\n",
        "\n",
        "# for regression, we use mean squared error. \n",
        "# for classification, we use cross entropy.\n",
        "# for now only mse is supported!\n",
        "lossFunction = 'mse' \n",
        "\n",
        "gdMethod = 'batch' # batch gradient descent method\n",
        "batchSize = 16 # only for minibatch gradient descent\n",
        "numEpochs = 4000 # number of epochs\n",
        "learningRate = [0.0005] # learning rates\n",
        "\n",
        "# for now only relu and sigmoid is supported\n",
        "lastActivationFunc = 'relu' # relu/sigmoid/softmax \n",
        "# last layer activation function, this one is important \n",
        "# because we need to use it for classification later\n",
        "\n",
        "crossValidationFlag = True # if you like to run cross validation, set this flag to True\n",
        "kFold = 5 # k-fold cross validation, at least need to be 2\n",
        "\n",
        "seed = 6565 # Do not change the seed for Assignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11bZSK6gaTmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fixSeed(seed=seed) # fix the seed of random generator to make sure comparision is possible "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNwon7HrgYlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some Useful Notes for those students who are interested to know more:\n",
        "'''\n",
        "- Neural networks are prone to overfitting.  Increasing the number of parameters \n",
        "  could lead to models that have complexity bigger than data. \n",
        "- Regularization, Normalization and Dropout are popular solutions to overfitting!\n",
        "- In a neural network, we usually use the softmax function as last layer \n",
        "  activation for multi-class classification and sigmoid for single class \n",
        "  classification.\n",
        "- For regression problems, we usually use Relu as last layer activation function \n",
        "  and MSE as the loss function that we want to minimize.\n",
        "- Cross-entropy is the most useful loss function for multi-class classification.\n",
        "- Sometimes we need to use multiple neurons in the output layer, which means \n",
        "  that we consider a neuron for each class.  In this case, we need to use \n",
        "  one-hot vectors to encode the labels. \n",
        "- Weight initialization is important! Gradient descent is not robust to \n",
        "  weight initialization! Xavier initialization is the most popular method \n",
        "  to initialize weights in neural networks.\n",
        "'''\n",
        "print('### MORE INFO HERE ###')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwllK9aJaV_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "colorBox = ['#377eb8','#FA0000','#344AA7', '#1EFA39','#00FBFF','#C500FF','#000000','#FFB600']\n",
        "\n",
        "if dataset == '2moons':\n",
        "  nSamples = 1000 if nSamples is None else nSamples\n",
        "  X,y = datasets.make_moons(n_samples=nSamples, noise=noise, random_state=seed)\n",
        "  numSamples, numFeatures, numClasses = X.shape[0], X.shape[1], 2\n",
        "\n",
        "  # shuffle X,y \n",
        "  idxList = list(range(nSamples))\n",
        "  random.shuffle(idxList) # inplace\n",
        "  X, y = X[idxList,:], y[idxList]\n",
        "\n",
        "elif dataset == 'frogsSmall' or dataset == 'frogs': \n",
        "\n",
        "  if dataset == 'frogs':\n",
        "    # original dataset\n",
        "    name = 'Frogs_MFCCs.csv'\n",
        "  else:\n",
        "    # a small subset of frogs original dataset, same as A2Q1\n",
        "    name = 'frogs-small.csv'\n",
        "\n",
        "  # check if we already have the file in the directory\n",
        "  if not path.isfile(name):\n",
        "    # otherwise ask user to upload it\n",
        "    print(\"Please upload {} in the current directory using choose files ...\".format(name))\n",
        "    # only in case of google colab\n",
        "    uploaded = files.upload() # choose frogs-small.csv\n",
        "    del uploaded\n",
        "\n",
        "  # just load the csv file\n",
        "  X = pd.read_csv(name, sep=',') \n",
        " \n",
        "  X[\"Family\"] = X[\"Family\"].astype('category')\n",
        "  X[\"FamilyCat\"] = X[\"Family\"].cat.codes # added to the last column\n",
        "  X, y = X.iloc[:,0:22].to_numpy(), X.iloc[:,-1].to_numpy()\n",
        "  \n",
        "  nSamples = X.shape[0] if nSamples is None else nSamples\n",
        "  X, y = X[:nSamples,:], y[:nSamples] # filter number of samples\n",
        "  \n",
        "  numSamples, numFeatures, numClasses = X.shape[0], X.shape[1], len(np.unique(y))\n",
        "\n",
        "print('#INFO: N (Number of Samples): {}, D (Number of Features): {}, C (Number of Classes): {}'.format(numSamples, numFeatures, numClasses))\n",
        "plt.figure()\n",
        "\n",
        "# if y min is not zero, make it zero\n",
        "y = y - y.min()\n",
        "assert y.min() == 0\n",
        "\n",
        "# sample required sample for visualization\n",
        "indices = list(range(numSamples))\n",
        "selectedIndices = np.random.choice(indices, visNumSamples)\n",
        "colors = [colorBox[y[idx]] for idx in selectedIndices]\n",
        "if numFeatures == 2:\n",
        "  XR = X[selectedIndices, :]\n",
        "else: \n",
        "  # use tsne to reduce dimensionality for visualization\n",
        "  XR = TSNE(n_components=2).fit_transform(X[selectedIndices,:])\n",
        "plt.scatter(XR[:, 0], XR[:, 1], s=10, color=colors)\n",
        "\n",
        "if len(y.shape) < 2:\n",
        "  y = np.expand_dims(y,-1) # shape of y should be N x 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSVYzx7tGPNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the network structure\n",
        "\n",
        "# # 2-Layer Network\n",
        "config = {\n",
        "    # Layer Name: [Number of Nodes (in and out), Bias, Activation Function]\n",
        "    'Hidden Layer 0':  [[numFeatures, 30], True, 'relu'], # w1\n",
        "    'Fully Connected': [[30, 1],  True, lastActivationFunc] # w2\n",
        "}\n",
        "\n",
        "# Two layer sigmoid\n",
        "# config = {\n",
        "#     # Layer Name: [Number of Nodes (in and out), Bias, Activation Function]\n",
        "#     'Hidden Layer 0':  [[numFeatures, 1000], True, 'sigmoid'], # w1\n",
        "#     'Fully Connected': [[1000, 1],  True, lastActivationFunc] # w2\n",
        "# }\n",
        "\n",
        "# 3-Layer Network\n",
        "# config = {\n",
        "#     # Layer Name: [Number of Nodes (in and out), Bias, Activation Function]\n",
        "#     'Hidden Layer 0':  [[numFeatures, 3], True, 'sigmoid'], # w1\n",
        "#     'Hidden Layer 1':  [[3, 5], True, 'sigmoid'], # w2\n",
        "#     'Fully Connected': [[5, 1],  True, lastActivationFunc] # w2\n",
        "# }\n",
        "\n",
        "# 4-layer Network\n",
        "config = {\n",
        "    # Layer Name: [Number of Nodes (in and out), Bias, Activation Function]\n",
        "    'Hidden Layer 0':  [[numFeatures, 100], True, 'relu'], # w1\n",
        "    'Hidden Layer 1':  [[100, 50], True, 'relu'], # w2\n",
        "    'Hidden Layer 2':  [[50, 5], True, 'relu'], # w3\n",
        "    'Fully Connected': [[5, 1],  True, lastActivationFunc] # w4\n",
        "}\n",
        "\n",
        "# config = {\n",
        "#     # Layer Name: [Number of Nodes (in and out), Bias, Activation Function]\n",
        "#     'Hidden Layer 0':  [[numFeatures, 30], True, 'relu'], # w1\n",
        "#     'Fully Connected': [[30, 1],  True, lastActivationFunc] # w4\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxbYCpZAaXcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fully Connected Neural Network Class\n",
        "class neuralNetwork():\n",
        "  \n",
        "    # initializing network\n",
        "    def __init__(self, config=None, numClass=2, learningRate=0.005, \n",
        "                 numEpochs=10, batchSize= 64, lossFunction='mse'):\n",
        "        self.config = config\n",
        "        self.configKeyList = list(self.config.keys())\n",
        "        self.lossFunction = lossFunction\n",
        "        self.numLayers = len(self.config)\n",
        "        self.layers = {} \n",
        "        self.layerShapes = {}\n",
        "        self.learningRate = learningRate\n",
        "        self.numEpochs = numEpochs\n",
        "        self.loss = []\n",
        "        self.lossT = []\n",
        "        self.acc = []\n",
        "        self.accT = []\n",
        "        self.batchSize = batchSize\n",
        "        self.numClass = numClass\n",
        "        \n",
        "        self.initWeights()\n",
        "        \n",
        "    # random init\n",
        "    def initWeights(self):\n",
        "        self.loss = []\n",
        "        self.lossT = []\n",
        "        self.acc = []\n",
        "        self.accT = []\n",
        "        if self.config != None:\n",
        "            for key in config:\n",
        "                # w is parameters, b is bias, a is activation function\n",
        "                self.layers[key] = {'W':np.random.randn(self.config[key][0][0], \n",
        "                                        self.config[key][0][1])/np.sqrt(self.config[key][0][1]), \n",
        "                                    'b':np.random.randn(self.config[key][0][1],\n",
        "                                        ) if self.config[key][1]==True else [], 'a':self.config[key][2]}\n",
        "                # keep track of shape only for better understanding\n",
        "                self.layerShapes[key] = {'IS':self.config[key][0][0],'OS':self.config[key][0][1],\n",
        "                                         'NP':np.prod(self.layers[key]['W'].shape)+len(self.layers[key]['b'])}\n",
        "        else:\n",
        "            raise '#Err: Make sure you set a configuration correctly!'\n",
        "    \n",
        "    # activation functions\n",
        "    def relu(self, X):\n",
        "        return np.maximum(0, X)\n",
        "    def sigmoid(self, X):\n",
        "        #TODO: fix the overflow problem in Numpy exp function\n",
        "        return 1./(1. + np.exp(-X))\n",
        "       \n",
        "    def activationFunc(self, X, type='sigmoid'):\n",
        "        if type == 'sigmoid':\n",
        "            return self.sigmoid(X)\n",
        "        elif type == 'relu':\n",
        "            return self.relu(X)\n",
        "        elif type == 'None':\n",
        "            return X # do nothing\n",
        "        else:\n",
        "            raise '#Err: Not implemented activation function!'\n",
        "            \n",
        "    # objective/loss/cost functions\n",
        "    def mse(self, y, yPred): # mean square error\n",
        "        return np.mean(np.power(y-yPred,2))\n",
        "\n",
        "    def lossFunc(self, y, yPred, type='mse'):\n",
        "        if type == 'mse':\n",
        "            return self.mse(y, yPred)\n",
        "        else:\n",
        "            raise '#Err: Not implemented objective function!'\n",
        "    \n",
        "    # back-propagation learning\n",
        "    # forward pass\n",
        "    def forward(self, X):\n",
        "        # apply a(W.T x X + b) for each layer\n",
        "        for key in config:\n",
        "            #print(X.shape, self.layers[key]['W'].shape)\n",
        "\n",
        "            # save input of each layer for backward pass\n",
        "            self.layers[key]['i'] = X \n",
        "            \n",
        "            z = np.dot(X, self.layers[key]['W']) \n",
        "            z = z + self.layers[key]['b'] if len(self.layers[key]['b'])!=0 else z\n",
        "\n",
        "            # save middle calculation for backward pass\n",
        "            self.layers[key]['z'] = z \n",
        "            X = self.activationFunc(z, type=self.layers[key]['a'])\n",
        "\n",
        "            # save middle calculation for backward pass\n",
        "            self.layers[key]['o'] = X \n",
        "\n",
        "        return X # yPred\n",
        "\n",
        "    # backward pass\n",
        "    def backward(self, y, yPred):\n",
        "\n",
        "        # derivative of sigmoid\n",
        "        def sigmoidPrime(x): \n",
        "            return self.sigmoid(x) * (1-self.sigmoid(x))\n",
        "\n",
        "        # derivative of relu\n",
        "        def reluPrime(x):\n",
        "            return np.where(x <= 0, 0, 1)\n",
        "\n",
        "        def identity(x):\n",
        "          return x\n",
        "        \n",
        "        #TODO: It's not necessary to use double for, \n",
        "        # it is possible to implement faster and more efficient version\n",
        "\n",
        "        # for each parameter (weights and bias) in each layer\n",
        "        for idx, key in enumerate(config):\n",
        "          # calculate derivatives\n",
        "          if self.layers[key]['a'] == 'sigmoid':\n",
        "              fPrime = sigmoidPrime  \n",
        "          elif self.layers[key]['a'] == 'relu':\n",
        "              fPrime = reluPrime  \n",
        "          elif self.layers[key]['a'] == 'softmax':\n",
        "              fPrime = softmaxPrime\n",
        "          else: # None \n",
        "              fPrime = identity\n",
        "          \n",
        "          deWRTdyPred = -(y-yPred) if self.lossFunction == 'mse' else 1 # de/dyPred\n",
        "          # print('de/dy')\n",
        "\n",
        "          # dyPred/dyPredBeforeActivation # in case of sigmoid g(x) x (1-g(x))\n",
        "          dyPredWRTdyPredPre = fPrime(self.layers[self.configKeyList[-1]]['o']) \n",
        "          # print('dy/dz')\n",
        "\n",
        "          # element wise multiplication/ hadamard product\n",
        "          delta = np.multiply(deWRTdyPred, dyPredWRTdyPredPre) \n",
        "          for idxW in range(len(config),idx,-1): # reverse\n",
        "            if idxW-1 == idx:\n",
        "              # calculating the derivative for the last one is different \n",
        "              # because it is respected to that specific weight\n",
        "              #print('\\nWeights of layer',idx)\n",
        "              deltaB = delta\n",
        "              dxWRTdW = self.layers[key]['i'].T # dxWRTdW\n",
        "              delta = np.dot(dxWRTdW,delta)\n",
        "              #print('dz/dw')\n",
        "            else:\n",
        "              # this loop is depended to the number of layers in the configuration\n",
        "              # print('\\nWeights of layer',idxW-1)\n",
        "              # the weights of current layer\n",
        "              # how fast the cost is changing as a function of the output activation\n",
        "              dxWRTdh = self.layers[self.configKeyList[idxW-1]]['W'].T # dxPreWRTdx-1\n",
        "              # print('dz/da')\n",
        "              # print('output of layer',idxW-1-1)\n",
        "              # the output of previous layer\n",
        "              # how fast the activation function is changing \n",
        "              dhWRTdhPre = fPrime(self.layers[self.configKeyList[idxW-1-1]]['o']) # dx-1WRTdx-1Pre\n",
        "              # print('da/dz')\n",
        "              delta = np.dot(delta, dxWRTdh) * dhWRTdhPre\n",
        "              \n",
        "          # sanity check: Numerical Gradient Checking\n",
        "          # f'(x) = lim (f(x+deltax)-f(x))/deltax when deltax -> 0\n",
        "          \n",
        "          # update parameters\n",
        "          # W = W - Gamma * dL/dW\n",
        "          self.layers[key]['djWRTdw'] = delta\n",
        "          self.layers[key]['W'] = self.layers[key]['W'] \\\n",
        "                                - self.learningRate/y.shape[0] * delta\n",
        "          # b = b - Gamma * dL/db\n",
        "          self.layers[key]['djWRTdb'] = deltaB\n",
        "          if len(self.layers[key]['b'])!=0:\n",
        "            self.layers[key]['b'] = self.layers[key]['b'] \\\n",
        "                    - self.learningRate/y.shape[0] * np.sum(deltaB, axis=0)\n",
        "    \n",
        "    # Utility Functions\n",
        "    def summary(self, space=20):\n",
        "        print('{: <{}} | {: <{}} | {: <{}} | {: <{}}'.format(\"Layer Name\", space, \n",
        "                                                      \"Input Shape\", space, \n",
        "                                                      \"Output Shape\", space, \n",
        "                                                      \"Number of Parameters\",space))\n",
        "        for key in config:\n",
        "            print('{: <{}} | {: <{}} | {: <{}} | {: <{}}'.format(key, space,\n",
        "                                        self.layerShapes[key]['IS'], space, \n",
        "                                        self.layerShapes[key]['OS'], space,\n",
        "                                        self.layerShapes[key]['NP'], space))\n",
        "    \n",
        "    def fit(self, X, y, XT=None, yT=None, method='batch', batchSize=None, numEpochs=None, \n",
        "            learningRate=None, initialState=None):\n",
        "      if numEpochs is None: # overwrite\n",
        "        numEpochs = self.numEpochs\n",
        "\n",
        "      if learningRate is not None:\n",
        "        self.learningRate = learningRate\n",
        "\n",
        "      if batchSize is not None:\n",
        "        self.batchSize = batchSize\n",
        "\n",
        "      # if initialState is not None:\n",
        "      #   # use the given initial parameters (weights and bias)\n",
        "      #   self.layers = initialState\n",
        "\n",
        "      if method == 'batch': \n",
        "        # this is infact mini-batch gradient descent, just for consistency in course material\n",
        "        # same as batched gradient descent in class to make it easier for you\n",
        "        pBar = tqdm(range(numEpochs))\n",
        "        for edx in pBar:\n",
        "          for idx in range(0, X.shape[0], self.batchSize):\n",
        "            start = idx\n",
        "            end = start + self.batchSize \n",
        "            end = end if end < X.shape[0] else X.shape[0]\n",
        "            \n",
        "            #TODO: Support variable batchsize\n",
        "            if end-start != self.batchSize:\n",
        "              continue\n",
        "            \n",
        "            x_, y_ = X[start:end, :], y[start:end, :]\n",
        "            yPred = self.forward(x_)\n",
        "            loss = self.lossFunc(y_, yPred, type=self.lossFunction)\n",
        "            self.backward(y_, yPred)\n",
        "          \n",
        "          yPred,yPredOrig = self.predict(X)\n",
        "          loss = self.lossFunc(y, yPredOrig, type=self.lossFunction)\n",
        "          self.loss.append(loss)\n",
        "          acc = self.accuracy(y, yPred)\n",
        "          self.acc.append(acc)\n",
        "\n",
        "          if XT is not None:\n",
        "            yPred, yPredOrig = self.predict(XT)\n",
        "            loss = self.lossFunc(yT, yPredOrig, type=self.lossFunction)\n",
        "            self.lossT.append(loss)\n",
        "            acc = self.accuracy(yT, yPred)\n",
        "            self.accT.append(acc)\n",
        "      else:\n",
        "        raise '#Err: {} Gradient Descent Method is Not implemented!'.format(method)\n",
        "    \n",
        "    def predict(self, X):\n",
        "\n",
        "        yPred = self.forward(X)\n",
        "        yPredOrigin = copy.deepcopy(yPred)\n",
        "\n",
        "        # last layer activation function, class prediction should be single \n",
        "        # and the output is between zero and one\n",
        "        if self.config[self.configKeyList[-1]][-1] == 'sigmoid': \n",
        "          yPred[yPred < 0.5] = 0\n",
        "          yPred[yPred >= 0.5] = 1\n",
        "        # multi-class problem\n",
        "        elif self.config[self.configKeyList[-1]][-1] == 'softmax': \n",
        "          raise '#Err: Prediction is not supported for softmax yet!'\n",
        "        elif self.config[self.configKeyList[-1]][-1] == 'relu': \n",
        "          yPred = np.round(yPred)\n",
        "          yPred = np.clip(yPred, 0, self.numClass-1) # sanity check \n",
        "\n",
        "        return yPred, yPredOrigin\n",
        "    \n",
        "    def error(self, y, yPred):\n",
        "        return self.lossFunc(y, yPred, type=self.lossFunction) \n",
        "        \n",
        "    def accuracy(self, y, yPred):\n",
        "        return 100*np.sum(y==yPred)/y.shape[0]\n",
        "\n",
        "    def plotLoss(self, loss=None, ax=None):\n",
        "      if loss is None:\n",
        "        loss = self.loss\n",
        "\n",
        "      if ax is None:\n",
        "        plt.plot(loss)\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Loss Per Epoch\")\n",
        "        plt.show()\n",
        "      else:\n",
        "        ax.plot(loss)\n",
        "        ax.set_xlabel(\"Epochs\")\n",
        "        ax.set_ylabel(\"Loss\")\n",
        "        ax.set_title(\"Loss Per Epoch\")\n",
        "\n",
        "    def crossValidationIndices(self, index, k=5):\n",
        "      # index is a list of indexes\n",
        "\n",
        "      cvList = []\n",
        "      for idx in range(k): # iterate over k-folds\n",
        "        interval = int(len(index)/k)\n",
        "        start = idx * interval\n",
        "        end = start + interval\n",
        "        testIndexes = list(range(start,end))\n",
        "        trainIndexes = list(range(0,start)) +  list(range(end,len(index)))\n",
        "\n",
        "        cvList.append((trainIndexes, testIndexes))\n",
        "\n",
        "      return cvList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzUwz92DUDaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if crossValidationFlag:\n",
        "  if len(learningRate) == 1:\n",
        "    fig, ax = plt.subplots(3,len(learningRate),figsize=(8,15))\n",
        "  else:\n",
        "    fig, ax = plt.subplots(3,len(learningRate),figsize=(30,3*(len(learningRate)+2)))\n",
        "else:\n",
        "  fig, ax = plt.subplots(1,1+len(learningRate),figsize=(30,1+len(learningRate)))\n",
        "\n",
        "for ldx, lr in enumerate(learningRate):\n",
        "  nn = neuralNetwork(config=config, numClass=numClasses, numEpochs=numEpochs, \n",
        "                   learningRate=lr, lossFunction=lossFunction)\n",
        "  # Initialize the network and the weights\n",
        "  nn.initWeights()\n",
        "\n",
        "  if crossValidationFlag:\n",
        "    indexes = list(range(X.shape[0]))\n",
        "    cvIndices = nn.crossValidationIndices(indexes, k=kFold)\n",
        "\n",
        "    accList = []\n",
        "    accTList = []\n",
        "    lossList = []\n",
        "    lossTList = []\n",
        "    for k in range(kFold):\n",
        "      nn.initWeights()\n",
        "      XTrain, yTrain = X[cvIndices[k][0],:], y[cvIndices[k][0],:]\n",
        "      XTest, yTest = X[cvIndices[k][1],:], y[cvIndices[k][1],:]\n",
        "      # Train the network\n",
        "      nn.fit(XTrain, yTrain, XTest, yTest, method=gdMethod, batchSize=batchSize, \n",
        "                  numEpochs=numEpochs, learningRate=lr)\n",
        "      accList.append(nn.acc)\n",
        "      accTList.append(nn.accT)\n",
        "      lossList.append(nn.loss)\n",
        "      lossTList.append(nn.lossT)\n",
        "\n",
        "    acc = np.mean(accList, axis=0)\n",
        "    accT = np.mean(accTList, axis=0)\n",
        "    loss = np.mean(lossList, axis=0)\n",
        "    lossT = np.mean(lossTList, axis=0)\n",
        "\n",
        "    print('Number of Epochs: {}, Learning Rate: {}, Dataset: {}'.format(numEpochs, lr, dataset))\n",
        "    print('Train Acc: {} Test Acc: {}, Error: {}, Test Error: {}'.format(acc[-1], accT[-1], loss[-1], lossT[-1]))\n",
        "      \n",
        "    # print the network structure\n",
        "    nn.summary()\n",
        "    yPred, yPredOrig = nn.predict(X)\n",
        "    print('#INFO: Mean squared error is {}'.format(nn.error(y,yPred)))\n",
        "\n",
        "    colors = [colorBox[int(yPred[idx])] for idx in selectedIndices]\n",
        "    if len(learningRate) == 1:\n",
        "        ax[2].scatter(XR[:, 0], XR[:, 1], s=10, color=colors)\n",
        "        ax[2].set_xlabel(\"X1\")\n",
        "        ax[2].set_ylabel(\"X2\")\n",
        "        ax[2].set_title(\"Data, LR: {}\".format(lr))\n",
        "\n",
        "        ax[0].plot(acc)\n",
        "        ax[0].plot(accT)\n",
        "        ax[0].legend(['Train','Test'])\n",
        "        ax[0].set_xlabel(\"Epochs\")\n",
        "        ax[0].set_ylabel(\"Accuracy\")\n",
        "        ax[0].set_title(\"Accuracy Per Epoch\"+\", LR: {}\".format(lr))\n",
        "\n",
        "        ax[1].plot(loss)\n",
        "        ax[1].plot(lossT)\n",
        "        ax[1].legend(['Train','Test'])\n",
        "        ax[1].set_xlabel(\"Epochs\")\n",
        "        ax[1].set_ylabel(\"Loss\")\n",
        "        ax[1].set_title(\"Loss Per Epoch\"+\", LR: {}\".format(lr)) \n",
        "    else:\n",
        "        ax[2,ldx].scatter(XR[:, 0], XR[:, 1], s=10, color=colors)\n",
        "        ax[2,ldx].set_xlabel(\"X1\")\n",
        "        ax[2,ldx].set_ylabel(\"X2\")\n",
        "        ax[2,ldx].set_title(\"Data, LR: {}\".format(lr))\n",
        "\n",
        "        ax[0,ldx].plot(acc)\n",
        "        ax[0,ldx].plot(accT)\n",
        "        ax[0,ldx].legend(['Train','Test'])\n",
        "        ax[0,ldx].set_xlabel(\"Epochs\")\n",
        "        ax[0,ldx].set_ylabel(\"Accuracy\")\n",
        "        ax[0,ldx].set_title(\"Accuracy Per Epoch\"+\", LR: {}\".format(lr))\n",
        "\n",
        "        ax[1,ldx].plot(loss)\n",
        "        ax[1,ldx].plot(lossT)\n",
        "        ax[1,ldx].legend(['Train','Test'])\n",
        "        ax[1,ldx].set_xlabel(\"Epochs\")\n",
        "        ax[1,ldx].set_ylabel(\"Loss\")\n",
        "        ax[1,ldx].set_title(\"Loss Per Epoch\"+\", LR: {}\".format(lr))\n",
        "  else:\n",
        "    # Perform a single run for visualization.\n",
        "    nn.fit(X, y, method=gdMethod, batchSize=batchSize, numEpochs=numEpochs, \n",
        "        learningRate=lr)\n",
        "    # print the network structure\n",
        "    nn.summary()\n",
        "    yPred, yPredOrig = nn.predict(X)\n",
        "    print('#INFO: Mean squared error is {}'.format(nn.error(y,yPred)))\n",
        "    \n",
        "    colors = [colorBox[int(yPred[idx])] for idx in selectedIndices]\n",
        "    ax[ldx+1].scatter(XR[:, 0], XR[:, 1], s=10, color=colors)\n",
        "    ax[ldx+1].set_xlabel(\"X1\")\n",
        "    ax[ldx+1].set_ylabel(\"X2\")\n",
        "    ax[ldx+1].set_title(\"LR: {}\".format(lr))\n",
        "\n",
        "    # Plot the mean squared error with respect to the nu\n",
        "    nn.plotLoss(ax=ax[0])\n",
        "\n",
        "    # train accuracy\n",
        "    acc = nn.accuracy(y.squeeze(-1),yPred.squeeze(-1))\n",
        "    print('#INFO: Train Accuracy is {}'.format(acc))\n",
        "\n",
        "if not crossValidationFlag:\n",
        "  ax[0].legend([\"LR: \"+str(lr) for lr in learningRate])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uKZw8MQ-edr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}